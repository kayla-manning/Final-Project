---
title: "Calculations"
author: "Kayla Manning"
date: "4/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(infer)
library(gganimate)

tidy_int_reg_bag <- read_csv("all_tidy.csv",
                             col_types = cols(col_date(format = ''),
                                             col_character(),
                                             col_character(),
                                             col_character(),
                                             col_character(),
                                             col_character(),
                                             col_double()))

```

```{r sd_total_swipes}

# looking at the standard deviation from year to year
# this is the grand total of swipes (resident, interhouse, etc.) at lunch for
# each house

tidy_int_reg_bag %>% 
  group_by(year, meal, house, type) %>% 
  filter(meal != "Brain Break",
         type == "grand_total",
         meal == "Lunch") %>% 
  summarise(iqr = IQR(count, na.rm = TRUE)) %>% 
  arrange(desc(iqr)) %>% 
  select(year, house, iqr) %>% 
  pivot_wider(names_from = year, values_from = iqr) %>% 
  mutate(iqr_increase = `1819` - `1718`) %>% 
  arrange(desc(iqr_increase))

```

```{r model_counts_by_house_day}

# overall, Tuesday has most swipes, while Saturday & Friday (intercept) have the
# least; Sunday is the only day without a significant difference from Friday
# swipe counts

tidy_int_reg_bag %>% 
  lm(count ~ day, data = .) %>% 
  tidy() %>%
  arrange(desc(estimate)) %>% 
  filter(p.value < 0.05)

# now looking at counts of interhouse swipes by days of the week; Wednesday has
# most interhouse swipes, while Saturday, Friday, and Sunday have the least
# Sunday has an insignificant difference from the baseline of Friday

tidy_int_reg_bag %>% 
  filter(type == "int") %>% 
  lm(count ~ day, data = .) %>% 
  tidy() %>% 
  arrange(desc(estimate)) %>% 
  filter(p.value < 0.05)

# interhouse swipes by meal; dinner actually has most 

tidy_int_reg_bag %>% 
  filter(type == "int",
         meal != "Day Total") %>% 
  lm(count ~ meal, data = .) %>% 
  tidy()

# lunch interhouse swipes by house

tidy_int_reg_bag %>% 
  filter(type == "int",
         meal == "Lunch",
         house != "FlyBy",
         house != "Hillel") %>% 
  lm(count ~ house, data = .) %>% 
  tidy() %>% 
  arrange(desc(estimate))

# lunch interhouse swipes by year; insignificant increase overall

tidy_int_reg_bag %>% 
  filter(type == "int",
         meal == "Lunch",
         house != "FlyBy",
         house != "Hillel") %>% 
  lm(count ~ year, data = .) %>% 
  tidy() %>% 
  arrange(desc(estimate))

# will look at fitted values for predicting average number of interhouse swipes
# in each house for each year and then calculated the difference between these
# two fitted values

tidy_int_reg_bag %>% 
  filter(type == "int",
         meal == "Lunch",
         house != "FlyBy",
         house != "Hillel") %>% 
  lm(count ~ year * house, data = .) %>% 
  augment() %>% 
  group_by(year, house) %>% 
  summarize(fitted = mean(.fitted)) %>% 
  arrange(desc(fitted)) %>% 
  pivot_wider(names_from = year, values_from = fitted) %>% 
  mutate(increase = `1819` - `1718`) %>% 
  arrange(desc(increase))

```

```{r trying_to_bootstrap}

# I want to create a function that randomly assigns a certain count of swipes to
# each house and then calculate the percentage that are interhouse. Then, I will
# plot the simulated distribution of interhouse swipes and then a line with the
# actual total interhouse swipes

set.seed(2020)
nreps <- 500

pct_int <- tidy_int_reg_bag %>% 
  filter(type %in% c("int", "grand_total")) %>% 
  pivot_wider(names_from = type, values_from = count) %>% 
  mutate(pct_int = int / grand_total * 100) %>% 
  select(house, year, pct_int)

boot <- tidy_int_reg_bag %>% 
  rep_sample_n(size = nrow(.), replace = TRUE, reps = nreps) %>% 
  group_by(replicate, type) %>% 
  summarise(avg_count = mean(count, na.rm = TRUE)) %>% 
  pivot_wider(names_from = type, values_from = avg_count) %>% 
  mutate(pct_int = int / grand_total * 100)

boot %>% 
  ggplot(aes(pct_int)) +
  geom_histogram(bins = 50) +
  scale_x_continuous() +
  geom_vline(xintercept = mean(pct_int %>% 
                                 filter(house == "Lowell") %>% 
                                 pull(pct_int), na.rm = TRUE),
             color = "blue") +
  geom_vline(xintercept = mean(pct_int %>% 
                                 filter(house == "Adams") %>% 
                                 pull(pct_int), na.rm = TRUE),
             color = "red") +
  geom_vline(xintercept = mean(pct_int %>% 
                                 filter(house == "Currier") %>% 
                                 pull(pct_int), na.rm = TRUE),
             color = "green") 



```


```{r trying_to_bootstrap_again}

# first I want to find how many total interhouse diners there are on any given
# day in each house (all three meals). there are 372 unique dates in my data
# set, so I want to randomly selected that many rows for each replicate and then
# randomly assign a house to each proportion.

set.seed(2019)

houses <- tidy_int_reg_bag %>% 
  select(house) %>% 
  filter(house != "Hillel",
         house != "FlyBy",
         house != "Annenberg") %>% 
  unique() %>% 
  pull()

pct <- tidy_int_reg_bag %>% 
  filter(type %in% c("int", "grand_total"),
         house != "Hillel",
         house != "FlyBy",
         house != "Annenberg") %>% 
  group_by(date, type, house) %>% 
  summarise(day_total = sum(count, na.rm = TRUE)) %>% 
  pivot_wider(names_from = type, values_from = day_total) %>% 
  mutate(pct_int = int / grand_total * 100) %>% 
  group_by(house) %>% 
  mutate(actual_pct_int = mean(pct_int))

pct %>% 
  select(date, pct_int, actual_pct_int) %>% 
  rep_sample_n(size = 372, replace = TRUE, reps = nreps) %>% 
  ungroup() %>% 
  mutate(houses = sample(houses, size = nrow(.), replace = TRUE)) %>% 
  
  # now I'm grouping by replicate and house to find the average percent of
  # interhouse diners in each house for each replicate
  
  group_by(replicate, houses) %>% 
  summarise(avg_pct_int = mean(pct_int, na.rm = TRUE)) %>% 
  
  # will add a column with the true average percentage of interhouse diners from
  # the original dataset so I can easily plot a vertical line on each of the
  # histograms that I'm about to generate
  
  mutate(actual_pct_int = pct %>% 
           group_by(house) %>% 
           summarise(actual_pct_int = mean(pct_int, na.rm = TRUE)) %>% 
           pull(actual_pct_int)) %>% 
  
  # next, I want to plot the values for each house on a histogram, with a
  # vertical line for the actual percent of interhouse diners for each house
  
  ggplot(aes(avg_pct_int, fill = houses)) +
  geom_histogram() +
  facet_wrap(~ houses) +
  geom_vline(aes(xintercept = actual_pct_int)) +
  
  
  # now adding/fixing labels to make the plot more appealing
  
  labs(title = "Bootstrapped Percentages of Interhouse versus Actual Percentages",
       fill = "Houses") +
  ylab("Count") +
  xlab("Simulated Median Percentage of Interhouse Swipes")
  
  

```



```{r currier_discussion_calculations}

# looking at the difference in grand_total median swipe counts between the two years

tidy_int_reg_bag %>% 
  filter(meal == "Lunch",
         house == "Currier",
         type == "grand_total") %>% 
  group_by(year) %>% 
  summarise(median = median(count)) %>% 
  pivot_wider(names_from = year, values_from = median) %>% 
  summarise(diff = `1819` - `1718`)

# now doing the same but for interhouse median swipe counts

tidy_int_reg_bag %>% 
  filter(meal == "Lunch",
         house == "Currier",
         type == "int") %>% 
  group_by(year) %>% 
  summarise(median = median(count, na.rm = TRUE)) %>% 
  pivot_wider(names_from = year, values_from = median) %>% 
  summarise(diff = `1819` - `1718`)

```

```{r dist_reg}

# adding distance in feet from John Harvard to the houses and neighborhood

dist_data <- tidy_int_reg_bag %>% 
  filter(house != "Annenberg",
         house != "Hillel",
         type %in% c("int", "grand_total")) %>% 
  mutate(distance = ifelse(house == "Adams", 1024.32, NA),
         distance = ifelse(house == "Quincy", 1436.10, distance),
         distance = ifelse(house == "Lowell", 1062.31, distance),
         distance = ifelse(house == "Leverett", 1750.81, distance),
         distance = ifelse(house == "Dunster", 2180.38, distance),
         distance = ifelse(house == "Mather", 2264.54, distance),
         distance = ifelse(house == "Winthrop", 1599.63, distance),
         distance = ifelse(house == "Eliot", 1877.21, distance),
         distance = ifelse(house == "Kirkland", 1740.05, distance),
         distance = ifelse(house == "Cabot", 3173.44, distance),
         distance = ifelse(house == "Currier", 3486.88, distance),
         distance = ifelse(house == "Pforzheimer", 3457.24, distance),
         distance = ifelse(house == "FlyBy", 641.54, distance),
         neighborhood = ifelse(house %in% c("Adams", "Quincy", "Lowell"), 
                               "River Central", NA),
         neighborhood = ifelse(house %in% c("Leverett", "Dunster", "Mather"),
                               "River West", neighborhood),
         neighborhood = ifelse(house %in% c("Winthrop", "Eliot", "Kirkland"), 
                               "River East", neighborhood),
         neighborhood = ifelse(house %in% c("Cabot", "Currier", "Pforzheimer"),
                               "Quad", neighborhood),
         neighborhood = ifelse(house == "FlyBy", "Yard", neighborhood)) %>% 
  group_by(house, year, type, distance, neighborhood) %>% 
  summarize(avg_count = median(count, na.rm = TRUE)) %>% 
  pivot_wider(names_from = type, values_from = avg_count)

# now I want to first make a linear model predicting total count from variables

dist_data %>% 
  lm(grand_total ~ distance, data = .) %>% 
  augment(newdata = dist_data, 
          ID = "house",
          type.predict = "response",
          type.residuals = "response") %>% 
  mutate(.resid = grand_total - .fitted) %>% 
  ggplot(aes(distance, grand_total, color = house)) +
  geom_point() +
  transition_manual(year)


tidy_int_reg_bag %>% 
  filter(house != "Annenberg",
         house != "Hillel",
         type %in% c("int", "grand_total")) %>% 
  mutate(distance = ifelse(house == "Adams", 1024.32, NA),
         distance = ifelse(house == "Quincy", 1436.10, distance),
         distance = ifelse(house == "Lowell", 1062.31, distance),
         distance = ifelse(house == "Leverett", 1750.81, distance),
         distance = ifelse(house == "Dunster", 2180.38, distance),
         distance = ifelse(house == "Mather", 2264.54, distance),
         distance = ifelse(house == "Winthrop", 1599.63, distance),
         distance = ifelse(house == "Eliot", 1877.21, distance),
         distance = ifelse(house == "Kirkland", 1740.05, distance),
         distance = ifelse(house == "Cabot", 3173.44, distance),
         distance = ifelse(house == "Currier", 3486.88, distance),
         distance = ifelse(house == "Pforzheimer", 3457.24, distance),
         distance = ifelse(house == "FlyBy", 641.54, distance),
         neighborhood = ifelse(house %in% c("Adams", "Quincy", "Lowell"), 
                               "River Central", NA),
         neighborhood = ifelse(house %in% c("Leverett", "Dunster", "Mather"),
                               "River West", neighborhood),
         neighborhood = ifelse(house %in% c("Winthrop", "Eliot", "Kirkland"), 
                               "River East", neighborhood),
         neighborhood = ifelse(house %in% c("Cabot", "Currier", "Pforzheimer"),
                               "Quad", neighborhood),
         neighborhood = ifelse(house == "FlyBy", "Yard", neighborhood)) %>% 
  group_by(house, type, distance, neighborhood, date) %>% 
  summarize(avg_count = median(count, na.rm = TRUE)) %>% 
  pivot_wider(names_from = type, values_from = avg_count) %>% 
  arrange(date) %>% 
  ggplot(aes(distance, grand_total, color = house)) +
  geom_point() +
  labs(subtitle = "Date: {frame_time}") +
  transition_time(date)


```

